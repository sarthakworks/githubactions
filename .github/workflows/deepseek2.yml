name: Code Review with DeepSeek-Coder R1

on:
  pull_request:
    branches:
      - main

jobs:
  code-review:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install torch transformers accelerate sentencepiece

      - name: Install Git LFS
        run: |
          sudo apt-get install git-lfs
          git lfs install

      - name: Cache DeepSeek-Coder R1 Model
        id: cache-model
        uses: actions/cache@v3
        with:
          path: deepseek-coder-1.3b-base
          key: deepseek-coder-1.3b-base-v1
          restore-keys: |
            deepseek-coder-1.3b-base-v1

      - name: Download DeepSeek-Coder R1 Model (Fallback to Hugging Face)
        if: steps.cache-model.outputs.cache-hit != 'true'
        run: |
          git lfs install
          git clone https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-base

      - name: Load Model and Tokenizer
        id: load-model
        run: |
          python -c "
          from transformers import AutoModelForCausalLM, AutoTokenizer;
          import pickle;
          model_path = './deepseek-coder-1.3b-base';
          tokenizer = AutoTokenizer.from_pretrained(model_path);
          model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto');
          with open('model_and_tokenizer.pkl', 'wb') as f:
              pickle.dump((model, tokenizer), f)
          "

      - name: Get changed files
        id: changed-files
        uses: tj-actions/changed-files@v35

      - name: Run Code Review
        id: code-review
        run: |
          python -c "
          import pickle;
          with open('model_and_tokenizer.pkl', 'rb') as f:
              model, tokenizer = pickle.load(f);

          import os;
          changed_files_string = os.environ['CHANGED_FILES'];
          changed_files = changed_files_string.split();

          review_comments = [];

          for file in changed_files:
              try:
                  with open(file, 'r') as f:
                      code = f.read();
                  inputs = tokenizer(f'Review the following code for optimization opportunities, including performance, algorithm efficiency, and unnecessary operations:\\n\\n{code}', return_tensors='pt').to('cpu');
                  outputs = model.generate(**inputs, max_new_tokens=512);
                  review = tokenizer.decode(outputs[0], skip_special_tokens=True);
                  if review.strip():
                    review_comments.append(f'**{file}:**\\n{review}\\n---\\n');
              except Exception as e:
                  review_comments.append(f'**Error reviewing {file}:** {e}\\n---\\n');

          with open('review_comments.txt', 'w') as f:
              f.write(''.join(review_comments));
          "
        env:
          CHANGED_FILES: ${{ steps.changed-files.outputs.all_changed_files }}

      - name: Post Code Review to Pull Request
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const comments = fs.readFileSync('./review_comments.txt', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comments
            });