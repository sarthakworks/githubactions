name: Code Review with DeepSeek-Coder R1

on:
  push:
    branches:
      - main
  pull_request:
    types: [opened, synchronize]

jobs:
  code-review:
    runs-on: ubuntu-latest  # Change to self-hosted runner if using GPU

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install torch transformers accelerate sentencepiece

      # Cache DeepSeek-Coder R1 Model
      - name: Cache DeepSeek-Coder R1 Model
        id: cache-model
        uses: actions/cache@v3
        with:
          path: deepseek-r1  # Path to where the model will be stored
          key: deepseek-r1-model-v1  # Cache key; can be versioned if the model changes
          restore-keys: |
            deepseek-r1-model-v1

      # Download DeepSeek-Coder R1 Model if not cached
      - name: Download DeepSeek-Coder R1 Model (Fallback to Hugging Face)
        if: steps.cache-model.outputs.cache-hit != 'true'
        run: |
          mkdir -p deepseek-r1
          cd deepseek-r1
          echo "Attempting to download model from Hugging Face..."
          git lfs install
          git clone https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-base
          cd deepseek-coder-1.3b-base

      - name: Run Code Review
        run: |
          python -c "
          from transformers import AutoModelForCausalLM, AutoTokenizer;
          model_path = './deepseek-r1';  # Path to model directory
          tokenizer = AutoTokenizer.from_pretrained(model_path);
          model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto');
          
          # Assuming files to review are in the repository
          files_to_review = ['file1.js', 'file2.js'];  # Adjust dynamically for changed files
          for file in files_to_review:
              with open(file, 'r') as f:
                  code = f.read();
              inputs = tokenizer(f'Review the following code for best practices:\n\n{code}', return_tensors='pt').to('cuda');
              outputs = model.generate(**inputs, max_new_tokens=512);
              print(tokenizer.decode(outputs[0], skip_special_tokens=True))
          "

      - name: Post Review as PR Comment
        uses: thollander/actions-comment-pull-request@v2
        with:
          message: "Code review complete! Check the output above for suggestions."
